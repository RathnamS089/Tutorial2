# -*- coding: utf-8 -*-
"""Tutorial2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NBW95TJNed1pvdHuN3zp72S5aJ9tD63W
"""

import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
  return 1/(1+np.exp(-x))
x=np.linspace(10,-10,100)
y_sigmoid=sigmoid(x)
plt.figure(figsize=(10,6))
plt.plot(x,y_sigmoid,label='sigmoid',color='red')
plt.title('Sigmoid activation function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.legend()
plt.show()

def tanh(x):
  return np.tanh(x)
x=np.linspace(10,-10,100)
y_tanh=tanh(x)
plt.figure(figsize=(10,6))
plt.plot(x,y_tanh,label='HYPERBOLIC FUNC',color='blue')
plt.title('Hyperbolic Activation Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.legend()
plt.show()

def relu(x):
  return np.maximum(0,x)
y_relu=relu(x)
plt.figure(figsize=(10,6))
plt.plot(x,y_relu,label='relu activation',color='green')
plt.title('RELU ACTIVATION')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.legend()
plt.show()

def leaky_relu(x,alpha=0.01):
  return np.where(x>0,x,x*alpha)
leaky_relu1=leaky_relu(x,0.01)
plt.figure(figsize=(10,6))
plt.plot(x,leaky_relu1,label='Leaky Rely',color='orange')
plt.title('Leaky relu Activation')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.legend()
plt.show()

def softmax(x):
  exp_x=np.exp(x-np.max(x))
  return exp_x/exp_x.sum(axis=0)
x_softmax=np.array([1.0,2.0,3.0,4.0])
y_softmax=softmax(x)
plt.figure(figsize=(10,6))
print(f"Softmax input {x_softmax}")
print(f"Softmax output {y_softmax}")
plt.plot(x,y_softmax,label='softmax func',color='red')
plt.title('Softmax Activation function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.legend()
plt.show()

plt.figure(figsize=(10,6))
plt.plot(x,y_sigmoid,label='sigmoid',color='red')
plt.plot(x,y_tanh,label='HYPERBOLIC FUNC',color='blue')
plt.plot(x,y_relu,label='relu activation',color='green')
plt.plot(x,leaky_relu1,label='Leaky Rely',color='orange')
plt.plot(x,y_softmax,label='softmax func',color='red')
plt.title('Comparision of activation functions')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.legend()
plt.show()

def loss_function(x):##simple loss function for y=x^2
  return x**2
def gradient(x):##slope for this curve dy/dx=2*x
  return 2*x

def gradient_descent(starting_point,learning_rate,iterations):
  x=starting_point
  history=[]
  for i in range(iterations):
    loss=loss_function(x)
    grad=gradient(x)
    history.append((x,loss))
    print(f"Iterartion:{i} x:{x:4f} Loss:{loss:.4f} Gradient:{grad:.4f}")
    x-=learning_rate*grad
  return history

starting_point=10
learning_rate=0.1
iterations=50
history=gradient_descent(starting_point,learning_rate,iterations)

x_values,loss_values=zip(*history)
x_plot=np.linspace(10,-10,500)
y_plot=loss_function(x_plot)
plt.figure(figsize=(10,6))
plt.plot(x_plot,y_plot,label='Loss Function y=x^2',color='red')
plt.scatter(x_values,loss_values,label='Gradient Descent Steps',color='blue')
plt.title('Loss Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.legend()
plt.grid()
plt.show()

import torch
import torch.nn as nn
from sklearn import datasets

??nn.Linear

m=nn.Linear(20,30)

m=nn.Linear(2,3)
input=torch.randn(3,2)
output=m(input)

input

print(output)
output.shape

m.weight.data

m.bias.data

def init_weight(m):
  if type(m) in [nn.Conv2D ,nn.linear]:
       m.weight.data=torch.tensor([[0.15,0.20],[0.25,0.30]])
       m.bias.data=torch.tensor([0.35])

class Perceptron(nn.Module):
  def __init__(self,h1,input_size,output_size):
    super().__init__()
    self.Linear=nn.Linear(input_size,h1)
    #manual input for initialization of weight of input  to the hidden node
    self.Linear.weight.data=torch.tensor([[0.15,0.20],[0.25,0.30]])
    self.Linear.bias.data=torch.tensor([0.35])
    self.Linear2=nn.Linear(h1,output_size)
    #manual input initialization of weight of input to the hidden node
    self.Linear2.weight.data=torch.tensor([[0.40,0.45],[0.50,0.55]])
    self.Linear2.bias.data=torch.tensor([0.60])
  def forward(self,x,print_values=True):
     net_h=self.Linear(x)
     out_h=torch.sigmoid(net_h)
     net_o=self.Linear2(out_h)
     out_o=torch.sigmoid(net_o)
     if print_values:
      print(f"net_h={net_h}")
      print(f"out_h={out_h}")
      print(f"net_o={net_o}")
      print(f"out_o={out_o}")
      return out_o

model=Perceptron(2,2,2)

print(model)

list(model.parameters())

model.forward(torch.Tensor([0.05,0.01]))

??nn.MSELoss
criterion=nn.MSELoss()
optimizer=torch.optim.SGD(model.parameters(),lr=0.5)
output=model.forward(torch.tensor([0.05,0.10]))
target=torch.tensor([0.01,0.99])
loss=criterion(output,target)
print(f"Total MSE ERROR:{loss.item()}")

#Backward pass
optimizer.zero_grad()
loss.backward()
optimizer.step()
print(list(model.parameters()))

print(model.forward(torch.tensor([0.06,0.12])))